{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLVC WS 2017\n",
    "\n",
    "Tutorial 9: Stochastic Gradient Descent and Learning Rate Dynamics\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Packages\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data:\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "BatchSize = 100\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./MNIST', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
    "                                          shuffle=True, num_workers=4) # Creating dataloader\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./MNIST', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
    "                                         shuffle=False, num_workers=4) # Creating dataloader\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three',\n",
    "           'four', 'five', 'six', 'seven', 'eight', 'nine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network:\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.Layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 256),\n",
    "            nn.ReLU())\n",
    "        self.Layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 10),\n",
    "            nn.LogSoftmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer1(x)\n",
    "        x = self.Layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net1 = NeuralNet()\n",
    "net2 = NeuralNet()\n",
    "net3 = NeuralNet()\n",
    "net4 = NeuralNet()\n",
    "\n",
    "if use_gpu:\n",
    "    net1 = net1.double().cuda()\n",
    "    net2 = net2.double().cuda()\n",
    "    net3 = net3.double().cuda()\n",
    "    net4 = net4.double().cuda()\n",
    "else:\n",
    "    net1 = net1.double()\n",
    "    net2 = net2.double()\n",
    "    net3 = net3.double()\n",
    "    net4 = net4.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with different Optimizer:\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Train(model,optimizer,IP,LB):\n",
    "    optimizer.zero_grad()\n",
    "    OP = model(IP)\n",
    "    loss = criterion(OP, LB)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=1e-4)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=1e-4, momentum=0.9)\n",
    "optimizer3 = optim.Adagrad(net3.parameters(), lr=1e-4)\n",
    "optimizer4 = optim.Adam(net4.parameters(), lr=1e-4)\n",
    "PlotAcc1 = []\n",
    "PlotAcc2 = []\n",
    "PlotAcc3 = []\n",
    "PlotAcc4 = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    correct3 = 0\n",
    "    correct4 = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28).double()).cuda(), Variable(labels).cuda()\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28).double()), Variable(labels)\n",
    "            \n",
    "        _ = Train(net1,optimizer1,inputs,labels)\n",
    "        _ = Train(net2,optimizer2,inputs,labels)\n",
    "        _ = Train(net3,optimizer3,inputs,labels)\n",
    "        _ = Train(net4,optimizer4,inputs,labels)\n",
    "        \n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28).double()).cuda(), labels.cuda()\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28).double()), labels\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        outputs = net1(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct1 += (predicted == labels).sum()\n",
    "        \n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct2 += (predicted == labels).sum()\n",
    "        \n",
    "        outputs = net3(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct3 += (predicted == labels).sum()\n",
    "        \n",
    "        outputs = net4(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct4 += (predicted == labels).sum()\n",
    "        \n",
    "    PlotAcc1.append(math.log10(correct1/float(total)))\n",
    "    PlotAcc2.append(math.log10(correct2/float(total)))\n",
    "    PlotAcc3.append(math.log10(correct3/float(total)))\n",
    "    PlotAcc4.append(math.log10(correct4/float(total)))\n",
    "    print('Epoch %d ; SGD Acc:  %f ; SGD with momentum Acc:  %f ; Adagrad Acc:  %f ; Adam Acc:  %f'%((epoch+1),correct1/float(total),correct2/float(total),correct3/float(total),correct4/float(total)))\n",
    "print('Finished Training')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "fig = plt.figure()        \n",
    "plt.plot(range(epoch+1),PlotAcc1,'r-',label='SGD')\n",
    "plt.plot(range(epoch+1),PlotAcc2,'c-',label='SGD with momentum')\n",
    "plt.plot(range(epoch+1),PlotAcc3,'g-',label='Adagrad')\n",
    "plt.plot(range(epoch+1),PlotAcc4,'b-',label='Adam')        \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Testing Accuracy (in log10 scale)')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network:\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.Layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 256),\n",
    "            nn.ReLU())\n",
    "        self.Layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 10),\n",
    "            nn.LogSoftmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer1(x)\n",
    "        x = self.Layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net1 = NeuralNet()\n",
    "net2 = NeuralNet()\n",
    "net3 = NeuralNet()\n",
    "\n",
    "if use_gpu:\n",
    "    net1 = net1.double().cuda()\n",
    "    net2 = net2.double().cuda()\n",
    "    net3 = net3.double().cuda()\n",
    "else:\n",
    "    net1 = net1.double()\n",
    "    net2 = net2.double()\n",
    "    net3 = net3.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Learning Rate Dynamics:\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=1e-2)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=1e-2)\n",
    "optimizer3 = optim.SGD(net3.parameters(), lr=1e-2)\n",
    "\n",
    "scheduler2 = optim.lr_scheduler.MultiStepLR(optimizer2, milestones=[3,7,9], gamma=0.9)\n",
    "scheduler3 = optim.lr_scheduler.ExponentialLR(optimizer3, gamma=0.99)\n",
    "\n",
    "Plotloss1 = []\n",
    "Plotloss2 = []\n",
    "Plotloss3 = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "    \n",
    "    loss1 = 0\n",
    "    loss2 = 0\n",
    "    loss3 = 0\n",
    "    scheduler2.step()\n",
    "    scheduler3.step()\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28).double()).cuda(), Variable(labels).cuda()\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28).double()), Variable(labels)\n",
    "            \n",
    "        loss1 += Train(net1,optimizer1,inputs,labels).data[0]\n",
    "        loss2 += Train(net2,optimizer2,inputs,labels).data[0]\n",
    "        loss3 += Train(net3,optimizer3,inputs,labels).data[0]   \n",
    "        \n",
    "    Plotloss1.append(loss1/(60000/BatchSize))\n",
    "    Plotloss2.append(loss2/(60000/BatchSize))\n",
    "    Plotloss3.append(loss3/(60000/BatchSize))\n",
    "    for opt in optimizer1.param_groups:\n",
    "        print('SGD Learning Rate: '+str(opt['lr']))\n",
    "    for opt in optimizer2.param_groups:\n",
    "        print('SGD (step decay) Learning Rate: '+str(opt['lr']))\n",
    "    for opt in optimizer3.param_groups:\n",
    "        print('SGD (step exp_decay) Learning Rate: '+str(opt['lr']))\n",
    "    print('Epoch %d ; SGD:  %f ; SGD step_decay:  %f ; SGD exp_decay:  %f'%((epoch+1),loss1/(60000/BatchSize),loss2/(60000/BatchSize),loss3/(60000/BatchSize)))\n",
    "    print('_______________________________________________________________')\n",
    "print('Finished Training')\n",
    "fig = plt.figure()        \n",
    "plt.plot(range(epoch+1),Plotloss1,'r-',label='SGD')\n",
    "plt.plot(range(epoch+1),Plotloss2,'g-',label='SGD with step_decay')     \n",
    "plt.plot(range(epoch+1),Plotloss3,'b-',label='SGD with exp_decay')  \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
